'use client';
import { useEffect, useState, useRef, useCallback, useTransition } from "react";
import { RealtimeAgent, RealtimeSession } from "@openai/agents/realtime";
import { generateUploadUrl } from "@/app/api/s3/s3";
import { v4 as uuidv4 } from "uuid";
import SmallHeader from "@/component/SmallHeader";
import BottomFixButton from "@/component/BottomFixButton";
import PlayIcon from "@/assets/icon/PlayIcon.svg";
import PauseIcon from "@/assets/icon/PauseIcon.svg";
import { usePathname, useRouter } from "next/navigation";
import { standardizeToMP3 } from "@/utils/audioPreprocessing";
import buildPatientInstructions from "./buildPrompt";
import { loadVirtualPatient, VirtualPatient } from "@/utils/loadVirtualPatient";
import LiveClientPopup from "@/component/LiveClientPopup";

type Props = { category: string; caseName: string };

const INITIAL_SECONDS = 12 * 60; // 12ë¶„
const INITIAL_READY_SECONDS = 60; // ì¤€ë¹„ì‹œê°„ 60ì´ˆ
const formatTemp = (t: number) => `${t.toFixed(1)}Â°C`;

export default function LiveCPXClient({ category, caseName }: Props) {
    const router = useRouter();

    // ===== ìƒíƒœ =====
    const [isRecording, setIsRecording] = useState(false);
    const [volume, setVolume] = useState(0);
    const [connected, setConnected] = useState(false);
    const [isUploading, setIsUploading] = useState(false);
    const [seconds, setSeconds] = useState(INITIAL_SECONDS);
    const [isFinished, setIsFinished] = useState(false);
    const [showPopup, setShowPopup] = useState(false);
    const [readySeconds, setReadySeconds] = useState<number | null>(null);
    const [statusMessage, setStatusMessage] = useState<string>();
    const [caseData, setCaseData] = useState<VirtualPatient | null>(null);
    const [isPending, startTransition] = useTransition();
    const [transcript, setTranscript] = useState<string>(""); // ğŸ“ ì „ì‚¬ë³¸

    const pathname = usePathname();

    // ===== ë ˆí¼ëŸ°ìŠ¤ =====
    const sessionRef = useRef<any>(null);
    const recorderRef = useRef<MediaRecorder | null>(null);
    const rafRef = useRef<number | null>(null);
    const mixedChunks = useRef<Blob[]>([]); // ğŸ§ ì‚¬ëŒ+AI í˜¼í•© ì˜¤ë””ì˜¤
    const audioCtxRef = useRef<AudioContext | null>(null);
    const destinationRef = useRef<MediaStreamAudioDestinationNode | null>(null);

    /** ì„¸ì…˜ ì •ë¦¬ */
    const stopAndResetSession = useCallback(async () => {
        try {
            if (sessionRef.current) {
                await (sessionRef.current as any).close?.();
                sessionRef.current = null;
            }
            if (recorderRef.current?.state === "recording") recorderRef.current.stop();
            recorderRef.current = null;
            cancelAnimationFrame(rafRef.current!);
            mixedChunks.current = [];
            setIsRecording(false);
            setConnected(false);
            setIsUploading(false);
            setIsFinished(false);
            setVolume(0);
            setSeconds(INITIAL_SECONDS);
            setTranscript("");
        } catch (err) {
            console.warn("ì„¸ì…˜ ì¢…ë£Œ ì˜¤ë¥˜:", err);
        }
    }, []);

    /** ì´ˆê¸°í™” */
    useEffect(() => {
        const isPopupShown = localStorage.getItem("isLiveClientShow");
        if (isPopupShown !== "false") setShowPopup(true);
    }, []);

    /** ì–¸ë§ˆìš´íŠ¸ ì‹œ ì •ë¦¬ */
    useEffect(() => {
        return () => { stopAndResetSession() };
    }, [stopAndResetSession]);

    /** caseData ë¡œë“œ */
    useEffect(() => {
        let mounted = true;
        (async () => {
            try {
                const data = await loadVirtualPatient(caseName);
                if (mounted) setCaseData(data);
            } catch (err) {
                console.error("ê°€ìƒí™˜ì ë¡œë“œ ì‹¤íŒ¨:", err);
            }
        })();
        return () => { mounted = false; };
    }, [caseName]);

    /** ğŸ§ ë³¼ë¥¨ í‘œì‹œ */
    const updateVolume = (analyser: AnalyserNode) => {
        const dataArray = new Uint8Array(analyser.frequencyBinCount);
        const loop = () => {
            analyser.getByteFrequencyData(dataArray);
            const avg = dataArray.reduce((a, b) => a + b, 0) / dataArray.length;
            setVolume(avg / 255);
            rafRef.current = requestAnimationFrame(loop);
        };
        loop();
    };

    /** íƒ€ì´ë¨¸ */
    useEffect(() => {
        if (!isRecording || isFinished) return;
        const id = setInterval(() => {
            setSeconds((s) => {
                if (s <= 1) {
                    clearInterval(id);
                    setIsRecording(false);
                    setIsFinished(true);
                    return 0;
                }
                return s - 1;
            });
        }, 1000);
        return () => clearInterval(id);
    }, [isRecording, isFinished]);

    useEffect(() => {
        if (seconds === 0 && !isUploading && isFinished && !isRecording) stopSession();
    }, [seconds, isUploading, isFinished]);

    /** ğŸ¤ ì„¸ì…˜ ì‹œì‘ */
    async function startSession() {
        if (sessionRef.current || connected || isRecording || isUploading) return;
        setConnected(true);

        try {
            const res = await fetch("/api/realtime-key");
            const { value } = await res.json();

            const agent = new RealtimeAgent({
                name: "í‘œì¤€í™” í™˜ì AI",
                instructions: buildPatientInstructions(caseData as VirtualPatient),
                voice: "ash",
            });

            const session: any = new RealtimeSession(agent, {
                model: "gpt-realtime-2025-08-28",
            });
            sessionRef.current = session;

            await session.connect({
                apiKey: value,
                speed: 1.5,
                prewarm: true,
                turnDetection: {
                    type: "client_vad",
                    silence_duration_ms: 0,
                    autoStart: false,
                },
            });

            // ğŸ§ AudioContext êµ¬ì„±
            const audioCtx = new AudioContext();
            const destination = audioCtx.createMediaStreamDestination();
            audioCtxRef.current = audioCtx;
            destinationRef.current = destination;

            // ğŸ™ ì‚¬ìš©ì ì…ë ¥
            const userStream = await navigator.mediaDevices.getUserMedia({ audio: true });
            const micSrc = audioCtx.createMediaStreamSource(userStream);
            const analyser = audioCtx.createAnalyser();
            micSrc.connect(analyser);
            micSrc.connect(destination);
            updateVolume(analyser);

            // ğŸ§ AI ì˜¤ë””ì˜¤ ì¶œë ¥ë„ destinationì— ì—°ê²°
            session.on("output_audio", (data: ArrayBuffer) => {
                const blob = new Blob([data], { type: "audio/mpeg" });
                const url = URL.createObjectURL(blob);
                const audio = new Audio(url);
                const source = audioCtx.createMediaElementSource(audio);
                source.connect(destination);
                audio.play();
            });

            // ì „ì‚¬ë³¸ ëˆ„ì 
            // ì „ì‚¬ë³¸ ëˆ„ì 
            session.on("message", (msg: any) => {
                console.log("ğŸ“© [MESSAGE RECEIVED]", msg); // âœ… ë©”ì‹œì§€ ì „ì²´ êµ¬ì¡° í™•ì¸ìš© ë¡œê·¸

                if (msg.role === "user") {
                    console.log("ğŸ‘¤ ì‚¬ìš©ì ë©”ì‹œì§€:", msg.content?.[0]?.text);
                    setTranscript((p) => p + `ì‚¬ìš©ì: ${msg.content?.[0]?.text ?? ""}\n`);
                } else if (msg.role === "assistant") {
                    console.log("ğŸ¤– ê°€ìƒí™˜ì ë©”ì‹œì§€:", msg.content?.[0]?.text);
                    setTranscript((p) => p + `ê°€ìƒí™˜ì: ${msg.content?.[0]?.text ?? ""}\n`);
                } else {
                    console.log("âš™ï¸ ê¸°íƒ€ ë©”ì‹œì§€ ì—­í• :", msg.role);
                }
            });

            session.on("response", (res: any) => {
                console.log("ğŸª„ [RESPONSE EVENT]", res);
            });


            // ğŸ¬ í†µí•© ì˜¤ë””ì˜¤ ë…¹ìŒ ì‹œì‘
            const mixedRecorder = new MediaRecorder(destination.stream, { mimeType: "audio/webm" });
            recorderRef.current = mixedRecorder;
            mixedRecorder.ondataavailable = (e) => {
                if (e.data.size > 0) mixedChunks.current.push(e.data);
            };
            mixedRecorder.start(500);
            setIsRecording(true);

        } catch (err) {
            console.error("ì„¸ì…˜ ì—°ê²° ì‹¤íŒ¨:", err);
            setConnected(false);
            alert("ì„¸ì…˜ ì—°ê²° ì‹¤íŒ¨ ë˜ëŠ” ë§ˆì´í¬ ì ‘ê·¼ ê±°ë¶€");
        }
    }

    /** â¹ ì„¸ì…˜ ì¢…ë£Œ + ì—…ë¡œë“œ */
    async function stopSession() {
        try {
            setIsUploading(true);
            setIsRecording(false);
            setIsFinished(true);
            if (recorderRef.current?.state === "recording") recorderRef.current.stop();
            if (sessionRef.current) await (sessionRef.current as any).close?.();
            cancelAnimationFrame(rafRef.current!);

            // í†µí•© ì˜¤ë””ì˜¤ MP3 ë³€í™˜
            const conversationBlob = new Blob(mixedChunks.current, { type: "audio/webm" });
            const conversationMP3 = await standardizeToMP3(conversationBlob);

            // ì „ì‚¬ë³¸ Blob
            const transcriptBlob = new Blob([transcript], { type: "text/plain" });

            // S3 ì—…ë¡œë“œ
            const bucket = process.env.NEXT_PUBLIC_S3_BUCKET_NAME!;
            const audioKey = `audio/conversation-${uuidv4()}.mp3`;
            const transcriptKey = `transcript/transcript-${uuidv4()}.txt`;

            const [audioUrl, txtUrl] = await Promise.all([
                generateUploadUrl(bucket, audioKey),
                generateUploadUrl(bucket, transcriptKey),
            ]);

            await Promise.all([
                fetch(audioUrl, { method: "PUT", headers: { "Content-Type": "audio/mpeg" }, body: conversationMP3 }),
                fetch(txtUrl, { method: "PUT", headers: { "Content-Type": "text/plain" }, body: transcriptBlob }),
            ]);

            startTransition(() => {
                router.push(`/score?s3Key=${encodeURIComponent(audioKey)}&caseName=${encodeURIComponent(caseName)}`);
            });
        } catch (err) {
            console.error("âŒ ì—…ë¡œë“œ ì˜¤ë¥˜:", err);
            alert("ì—…ë¡œë“œ ì‹¤íŒ¨");
        } finally {
            setConnected(false);
            setIsUploading(false);
        }
    }

    const toggleRecording = () => {
        if (isRecording) {
            setStatusMessage("ê°€ìƒí™˜ìì™€ì˜ ëŒ€í™”ëŠ” ì¼ì‹œì •ì§€í•  ìˆ˜ ì—†ì–´ìš”");
            return;
        }
        if (!connected) startSession();
        else stopSession();
    };

    // UI ====
    const vitalData = caseData?.properties.meta.vitals;
    const showTime = (sec: number) => `${String(Math.floor(sec / 60)).padStart(2, "0")}:${String(sec % 60).padStart(2, "0")}`;

    useEffect(() => {
        if (statusMessage) {
            const timer = setTimeout(() => setStatusMessage(undefined), 3000);
            return () => clearTimeout(timer);
        }
    }, [statusMessage]);

    const handleReadyStart = () => {
        setShowPopup(false);
        setReadySeconds(INITIAL_READY_SECONDS);
    };

    useEffect(() => {
        if (readySeconds === null) return;
        if (readySeconds > 0) {
            const id = setInterval(() => setReadySeconds((p) => (p !== null ? p - 1 : null)), 1000);
            return () => clearInterval(id);
        } else if (readySeconds === 0) {
            startSession();
            setReadySeconds(null);
        }
    }, [readySeconds]);

    return (
        <div className="flex flex-col min-h-dvh">
            {showPopup && <LiveClientPopup onClose={() => setShowPopup(false)} onReadyStart={handleReadyStart} />}
            <div className="flex flex-col">
                <SmallHeader title={`${category} | ${caseName}`} onClick={() => router.push("/live-select")} />

                {/* ì„¤ëª… */}
                <div className="px-8 pt-4">
                    <p className="text-[#210535] text-[18px] leading-relaxed">{caseData?.description}</p>
                </div>

                {/* ë°”ì´íƒˆ */}
                <div className="grid grid-cols-2 gap-y-4 gap-x-4 px-8 pt-4 pb-6">
                    <div className="flex gap-2"><div className="font-semibold">í˜ˆì••</div><div>{vitalData?.bp}</div></div>
                    <div className="flex gap-2"><div className="font-semibold">ë§¥ë°•</div><div>{vitalData?.hr}</div></div>
                    <div className="flex gap-2"><div className="font-semibold">í˜¸í¡ìˆ˜</div><div>{vitalData?.rr}</div></div>
                    <div className="flex gap-2"><div className="font-semibold">ì²´ì˜¨</div><div>{formatTemp(Number(vitalData?.bt))}</div></div>
                </div>

                {/* ì¤‘ì•™ */}
                <div className="px-8 flex-1 pb-[136px] flex flex-col items-center justify-center gap-[12px] relative overflow-hidden">
                    <div className="relative">
                        {isRecording && (
                            <div className="absolute rounded-full transition-transform duration-100 ease-out"
                                style={{
                                    width: "206px", height: "206px", top: "49%", left: "50%",
                                    transform: `translate(-50%, -50%) scale(${1 + volume * 1.5})`,
                                    opacity: 0.3, background: "radial-gradient(circle, #B1A5E8, #BBA6FF 80%, transparent)",
                                    boxShadow: `0 0 ${40 + volume * 50}px #B1A5E8`,
                                }}
                            ></div>
                        )}
                        <button
                            type="button"
                            onClick={toggleRecording}
                            className="outline-none relative cursor-pointer hover:opacity-70 transition-transform duration-150 ease-out active:scale-90"
                            disabled={isUploading || connected || isFinished}
                        >
                            {isRecording ? (
                                <PauseIcon className="w-[240px] h-[240px] text-[#7553FC] opacity-70" />
                            ) : (
                                <PlayIcon className="w-[240px] h-[240px] text-[#7553FC]" />
                            )}
                        </button>
                    </div>

                    {/* íƒ€ì´ë¨¸ */}
                    <div className="font-semibold text-[#7553FC]">
                        {readySeconds !== null && !isRecording && !isFinished ? (
                            <div className="text-center">
                                <span className="text-[36px]">{readySeconds}ì´ˆ</span>
                                <span className="font-medium text-[20px]"><br />í›„ ì‹¤ìŠµì´ ì‹œì‘ë©ë‹ˆë‹¤.<br />ì¤€ë¹„ë˜ì—ˆë‹¤ë©´ <span className="font-bold">í”Œë ˆì´ ë²„íŠ¼</span>ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”.</span>
                            </div>
                        ) : (
                            <span className="text-[36px]">{showTime(seconds)}</span>
                        )}
                    </div>
                </div>

                <BottomFixButton
                    disabled={isUploading || seconds == INITIAL_SECONDS}
                    buttonName={"ì¢…ë£Œ ë° ì±„ì í•˜ê¸°"}
                    onClick={stopSession}
                    loading={isPending || isUploading}
                />

                {statusMessage && (
                    <div className="fixed bottom-30 left-1/2 -translate-x-1/2 bg-[#c7beeeff] text-[#210535] text-[18px] font-medium 
              px-4 py-3 rounded-xl shadow-lg flex z-[100] animate-slideUpFade justify-center items-center w-[calc(100%-40px)]">
                        {statusMessage}
                    </div>
                )}
            </div>
        </div>
    );
}
